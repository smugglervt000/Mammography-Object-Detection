{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torchvision.ops import nms, box_iou\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "from anchor_utils import AnchorGenerator as AnchorGenerator\n",
    "from torchvision.models.detection.rpn import AnchorGenerator as AnchorGeneratorRCNN, RPNHead\n",
    "\n",
    "from model import RetinaDataset, RetinaNet, collate\n",
    "\n",
    "from faster_rcnn import RCNNDataset, FasterRCNNModel\n",
    "from faster_rcnn import collate as collate_rcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def evaluate_model_rcnn(model, dataloader, device, iou_threshold=0.5, score_threshold=0.):\n",
    "    \"\"\"\n",
    "    Evaluates Faster R-CNN model on the test data and returns the mAP, IoU, and specificity.\n",
    "\n",
    "    Args:\n",
    "        model: Model to evaluate.\n",
    "        dataloader: Dataloader of test data.\n",
    "        device: Device to use for evaluation.\n",
    "        iou_threshold: The IoU threshold for NMS.\n",
    "        score_threshold: Confidence score threshold for filtering predictions.\n",
    "\n",
    "    Returns:\n",
    "        final_map: Computed mAP metrics\n",
    "        average_iou: Average IoU score.\n",
    "        specificity: Specificity. \n",
    "    \"\"\"\n",
    "    map_metric = MeanAveragePrecision(extended_summary=True)\n",
    "    iou_scores = []\n",
    "\n",
    "    tn = 0  \n",
    "    fp = 0  \n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = images.to(device)\n",
    "            targets = {k: [v.to(device) for v in t] for k, t in targets.items()}\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Convert to the expected format for the metric\n",
    "            formatted_targets = [{'boxes': b, 'labels': l.int()} for b, l in zip(targets['boxes'], targets['labels'])]\n",
    "\n",
    "            preds = []\n",
    "            for output in outputs:\n",
    "                boxes = output['boxes']\n",
    "                scores = output['scores']\n",
    "                labels = output['labels']\n",
    "\n",
    "                high_conf_indices = scores > score_threshold\n",
    "                boxes = boxes[high_conf_indices]\n",
    "                scores = scores[high_conf_indices]\n",
    "                labels = labels[high_conf_indices]\n",
    "\n",
    "                # Ensure boxes, scores, and labels are tensors\n",
    "                if isinstance(boxes, list):\n",
    "                    boxes = torch.tensor(boxes, device=device)\n",
    "                if isinstance(scores, list):\n",
    "                    scores = torch.tensor(scores, device=device)\n",
    "                if isinstance(labels, list):\n",
    "                    labels = torch.tensor(labels, device=device)\n",
    "\n",
    "                if boxes.numel() > 0:  # Check if there are any boxes\n",
    "                    # Apply NMS\n",
    "                    keep = nms(boxes, scores, iou_threshold)\n",
    "                    boxes = boxes[keep]\n",
    "                    scores = scores[keep]\n",
    "                    labels = labels[keep]\n",
    "\n",
    "                    preds.append({\n",
    "                        'boxes': boxes,\n",
    "                        'labels': labels.int(),\n",
    "                        'scores': scores\n",
    "                    })\n",
    "                else:\n",
    "                    # If there are no boxes, just append an empty prediction\n",
    "                    preds.append({\n",
    "                        'boxes': torch.empty((0, 4), device=device),\n",
    "                        'labels': torch.empty((0,), dtype=torch.int64, device=device),\n",
    "                        'scores': torch.empty((0,), device=device)\n",
    "                    })\n",
    "\n",
    "            # Evaluate negative samples\n",
    "            for target, pred in zip(formatted_targets, preds):\n",
    "                if target['boxes'].numel() == 0:  \n",
    "                    if len(pred['scores']) > 0:  \n",
    "                        high_conf_indices = pred['scores'] > 0.2\n",
    "                        if high_conf_indices.sum() == 0:\n",
    "                            tn += 1  \n",
    "                        else:\n",
    "                            fp += 1  \n",
    "                    else:\n",
    "                        tn += 1  \n",
    "                else:\n",
    "                    # Update the metric with the current batch\n",
    "                    map_metric.update([pred], [target])\n",
    "\n",
    "                    # Calculate IoU for each image\n",
    "                    if len(pred['boxes']) > 0 and len(target['boxes']) > 0:\n",
    "                        iou = box_iou(pred['boxes'], target['boxes']).diag().mean().item()\n",
    "                    else:\n",
    "                        iou = 0.0\n",
    "                    iou_scores.append(iou)\n",
    "\n",
    "    # Compute the final mAP score\n",
    "    final_map = map_metric.compute()\n",
    "    average_iou = sum(iou_scores) / len(iou_scores) if iou_scores else 0.0\n",
    "\n",
    "    # Calculate specificity: TP / (TP + FP)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 1.0\n",
    "\n",
    "    print(f\"Mean Average Precision (mAP): {final_map['map']}\")\n",
    "    print(f\"Mean Average Precision (mAP_50): {final_map['map_50']}\")\n",
    "    print(f\"Average IoU: {average_iou}\")\n",
    "    print(f\"Precision on negative samples: {specificity:.4f}\")\n",
    "\n",
    "    return final_map, average_iou, specificity\n",
    "\n",
    "\n",
    "def evaluate_model_retina(model, dataloader, device, iou_threshold=0.5, score_threshold=0.):\n",
    "    \"\"\"\n",
    "    Evaluates RetinaNet model on the test data and returns the mAP, IoU, and specificity.\n",
    "\n",
    "    Args:\n",
    "        model: Model to evaluate.\n",
    "        dataloader: Dataloader of test data.\n",
    "        device: Device to use for evaluation.\n",
    "        iou_threshold: The IoU threshold for NMS.\n",
    "        score_threshold: Confidence score threshold for filtering predictions.\n",
    "\n",
    "    Returns:\n",
    "        final_map: Computed mAP metrics\n",
    "        average_iou: Average IoU score.\n",
    "        specificity: Specificity. \n",
    "    \"\"\"\n",
    "    map_metric = MeanAveragePrecision(extended_summary=True)\n",
    "    iou_scores = []\n",
    "\n",
    "    tn = 0  \n",
    "    fp = 0  \n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = images.to(device)\n",
    "            targets = {k: [v.to(device) for v in t] for k, t in targets.items()}\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Convert to the expected format for the metric\n",
    "            formatted_targets = [{'boxes': b, 'labels': l.int()} for b, l in zip(targets['bbox'], targets['labels'])]\n",
    "\n",
    "            preds = []\n",
    "            for output in outputs[1]:\n",
    "                boxes = output['bbox']\n",
    "                scores = output['scores']\n",
    "                labels = output['labels']\n",
    "\n",
    "                high_conf_indices = scores > score_threshold\n",
    "                boxes = boxes[high_conf_indices]\n",
    "                scores = scores[high_conf_indices]\n",
    "                labels = labels[high_conf_indices]\n",
    "\n",
    "                # Ensure boxes, scores, and labels are tensors\n",
    "                if isinstance(boxes, list):\n",
    "                    boxes = torch.tensor(boxes, device=device)\n",
    "                if isinstance(scores, list):\n",
    "                    scores = torch.tensor(scores, device=device)\n",
    "                if isinstance(labels, list):\n",
    "                    labels = torch.tensor(labels, device=device)\n",
    "\n",
    "                if boxes.numel() > 0:  # Check if there are any boxes\n",
    "                    # Apply NMS\n",
    "                    keep = nms(boxes, scores, iou_threshold)\n",
    "                    boxes = boxes[keep]\n",
    "                    scores = scores[keep]\n",
    "                    labels = labels[keep]\n",
    "\n",
    "                    preds.append({\n",
    "                        'boxes': boxes,\n",
    "                        'labels': labels.int(),\n",
    "                        'scores': scores\n",
    "                    })\n",
    "                else:\n",
    "                    # If there are no boxes, just append an empty prediction\n",
    "                    preds.append({\n",
    "                        'boxes': torch.empty((0, 4), device=device),\n",
    "                        'labels': torch.empty((0,), dtype=torch.int64, device=device),\n",
    "                        'scores': torch.empty((0,), device=device)\n",
    "                    })\n",
    "\n",
    "            # Evaluate negative samples\n",
    "            for target, pred in zip(formatted_targets, preds):\n",
    "                if target['boxes'].numel() == 0:  \n",
    "                    if len(pred['scores']) > 0:  \n",
    "                        high_conf_indices = pred['scores'] > 0.2\n",
    "                        if high_conf_indices.sum() == 0:\n",
    "                            tn += 1  \n",
    "                        else:\n",
    "                            # for box in pred['boxes']: # Keep this?\n",
    "                            fp += 1  \n",
    "                    else:\n",
    "                        tn += 1  \n",
    "                else:\n",
    "                    # Update the metric with the current batch\n",
    "                    map_metric.update([pred], [target])\n",
    "\n",
    "                    # Calculate IoU for each image\n",
    "                    if len(pred['boxes']) > 0 and len(target['boxes']) > 0:\n",
    "                        iou = box_iou(pred['boxes'], target['boxes']).diag().mean().item()\n",
    "                    else:\n",
    "                        iou = 0.0\n",
    "                    iou_scores.append(iou)\n",
    "\n",
    "    # Compute the final mAP score\n",
    "    final_map = map_metric.compute()\n",
    "    average_iou = sum(iou_scores) / len(iou_scores) if iou_scores else 0.0\n",
    "\n",
    "    # Calculate negative sample precision: TP / (TP + FP)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 1.0\n",
    "\n",
    "    print(f\"Mean Average Precision (mAP): {final_map['map']}\")\n",
    "    print(f\"Mean Average Precision (mAP_50): {final_map['map_50']}\")\n",
    "    print(f\"Average IoU: {average_iou}\")\n",
    "    print(f\"Precision on negative samples: {specificity:.4f}\")\n",
    "\n",
    "    return final_map, average_iou, specificity\n",
    "\n",
    "\n",
    "def evaluate_multi_retina(model, dataloader, device, iou_threshold=0.5, score_threshold=0.0):\n",
    "    \"\"\"\n",
    "    Evaluates the given model on the test data and returns the Mean Average Precision (mAP) for images with targets,\n",
    "    and the precision for negative samples (images without targets). Also calculates precision, recall, and F1-score\n",
    "    per label.\n",
    "\n",
    "    Args:\n",
    "        model: The model to evaluate.\n",
    "        dataloader: DataLoader containing the test data.\n",
    "        device: The device (CPU or GPU) to use for evaluation.\n",
    "        iou_threshold: The IoU threshold for Non-Max Suppression (NMS) and true positive determination.\n",
    "        score_threshold: The confidence score threshold for filtering predictions.\n",
    "\n",
    "    Returns:\n",
    "        final_map: The computed Mean Average Precision (mAP) score.\n",
    "        average_iou: The average IoU score.\n",
    "        ns_tn_rate: Precision on negative samples.\n",
    "        class_metrics: Dictionary with precision, recall, and F1-score for each label.\n",
    "    \"\"\"\n",
    "    \n",
    "    map_metric = MeanAveragePrecision(extended_summary=True)\n",
    "    iou_scores = []\n",
    "    \n",
    "    tn, fp = 0, 0  # For negative sample precision\n",
    "\n",
    "    # Track true positives, false positives, and false negatives for each class\n",
    "    per_class_tp = defaultdict(int)\n",
    "    per_class_fp = defaultdict(int)\n",
    "    per_class_fn = defaultdict(int)\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = images.to(device)\n",
    "            targets = {k: [v.to(device) for v in t] for k, t in targets.items()}\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Convert targets into the required format for the metric\n",
    "            formatted_targets = [{'boxes': b, 'labels': l.int()} for b, l in zip(targets['bbox'], targets['labels'])]\n",
    "\n",
    "            preds = []\n",
    "            for output in outputs[1]:\n",
    "                boxes = output['bbox']\n",
    "                scores = output['scores']\n",
    "                labels = output['labels']\n",
    "\n",
    "                if isinstance(boxes, list):\n",
    "                    boxes = torch.tensor(boxes, device=device)\n",
    "                if isinstance(scores, list):\n",
    "                    scores = torch.tensor(scores, device=device)\n",
    "                if isinstance(labels, list):\n",
    "                    labels = torch.tensor(labels, device=device)\n",
    "\n",
    "                if boxes.numel() > 0:  # If there are predicted boxes\n",
    "                    # Apply Non-Max Suppression (NMS)\n",
    "                    keep = nms(boxes, scores, iou_threshold)\n",
    "                    boxes = boxes[keep]\n",
    "                    scores = scores[keep]\n",
    "                    labels = labels[keep]\n",
    "\n",
    "                    preds.append({\n",
    "                        'boxes': boxes,\n",
    "                        'labels': labels.int(),\n",
    "                        'scores': scores\n",
    "                    })\n",
    "                else:\n",
    "                    preds.append({\n",
    "                        'boxes': torch.empty((0, 4), device=device),\n",
    "                        'labels': torch.empty((0,), dtype=torch.int64, device=device),\n",
    "                        'scores': torch.empty((0,), device=device)\n",
    "                    })\n",
    "\n",
    "            for target, pred in zip(formatted_targets, preds):\n",
    "                if target['boxes'].numel() == 0:  # If no ground-truth boxes (negative samples)\n",
    "                    if len(pred['scores']) > 0:  # Check if there are predictions\n",
    "                        high_conf_indices = pred['scores'] > score_threshold\n",
    "                        if high_conf_indices.sum() == 0:\n",
    "                            tn += 1\n",
    "                        else:\n",
    "                            fp += 1\n",
    "                    else:\n",
    "                        tn += 1\n",
    "                else:\n",
    "                    # Update the mAP metric with the current batch\n",
    "                    map_metric.update([pred], [target])\n",
    "\n",
    "                    # Calculate IoU for valid boxes\n",
    "                    if len(pred['boxes']) > 0 and len(target['boxes']) > 0:\n",
    "                        iou_matrix = box_iou(pred['boxes'], target['boxes'])\n",
    "                        max_ious, max_indices = iou_matrix.max(dim=1)  # Get the best matching ground truth for each prediction\n",
    "\n",
    "                        # Calculate the average IoU regardless of whether it's above the threshold\n",
    "                        for iou in max_ious:\n",
    "                            iou_scores.append(iou.item())  # Append IoU for averaging\n",
    "\n",
    "                        # Filter TPs based on IoU threshold\n",
    "                        for pred_idx, iou in enumerate(max_ious):\n",
    "                            pred_label = pred['labels'][pred_idx]\n",
    "                            gt_label = target['labels'][max_indices[pred_idx]]\n",
    "\n",
    "                            if iou > iou_threshold and pred_label == gt_label:\n",
    "                                per_class_tp[pred_label.item()] += 1\n",
    "                            else:\n",
    "                                per_class_fp[pred_label.item()] += 1\n",
    "\n",
    "                        # Ground truth boxes that weren't matched to any prediction\n",
    "                        for gt_label in target['labels']:\n",
    "                            if gt_label not in pred['labels'][max_indices]:\n",
    "                                per_class_fn[gt_label.item()] += 1\n",
    "                    else:\n",
    "                        iou_scores.append(0.0)  # No predictions or ground truths, IoU = 0\n",
    "\n",
    "\n",
    "    # Compute final mAP score\n",
    "    final_map = map_metric.compute()\n",
    "    average_iou = sum(iou_scores) / len(iou_scores) if iou_scores else 0.0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 1.0\n",
    "\n",
    "    # Calculate precision, recall, and F1-score for each label\n",
    "    class_metrics = {}\n",
    "    for label in set(list(per_class_tp.keys()) + list(per_class_fp.keys()) + list(per_class_fn.keys())):\n",
    "        tp = per_class_tp[label]\n",
    "        fp = per_class_fp[label]\n",
    "        fn = per_class_fn[label]\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        class_metrics[label] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score\n",
    "        }\n",
    "\n",
    "    # Print or log the results\n",
    "    print(f\"Mean Average Precision (mAP): {final_map['map']:.4f}\")\n",
    "    print(f\"Mean Average Precision (mAP@50): {final_map['map_50']:.4f}\")\n",
    "    print(f\"Average IoU: {average_iou:.4f}\")\n",
    "    print(f\"Precision on negative samples: {specificity:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer Label Metrics:\")\n",
    "    for label, metrics in class_metrics.items():\n",
    "        print(f\"Label {label}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1: {metrics['f1_score']:.4f}\")\n",
    "\n",
    "    return final_map, average_iou, specificity, class_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of what to run to evaluate model (in this case we evaluate RetinaNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoint path to model you want to evaluate\n",
    "checkpoint_path = '/vol/biomedic3/bglocker/mscproj24/mrm123/slurm_scripts/Retinanet Models UE0ns/4mb70yay/checkpoints/best_unfiltered_embed_0ns_0.ckpt'\n",
    "model = RetinaNet.load_from_checkpoint(checkpoint_path, ratios=[1.0, 1.1912243160876392, 0.83947245409187], scales=[0.6701667817494404, 0.43826872391648763, 1.0929571608034148])\n",
    "\n",
    "test_dataset = RetinaDataset(csv_file='csv_files/unfiltered_embed_0ns/test.csv', augmentation=False)\n",
    "test_loader = utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "\n",
    "final_map, average_iou, ns_tn_rate = evaluate_model_retina(model, test_loader, device, iou_threshold=0.5, score_threshold=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a function to calculate the mean and uncertainty\n",
    "def calculate_mean_uncertainty(metrics):\n",
    "    \"\"\"\n",
    "    Function to calculate uncertainty using multiple metrics from multiple of the same models trained\n",
    "\n",
    "    Args: \n",
    "        metrics: dictionary to metrics to evaluate\n",
    "\n",
    "    Returns:\n",
    "        results: dictionary of mean and std per metric (using metric as key)\n",
    "    \n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for key, values in metrics.items():\n",
    "        # Handle map key separately\n",
    "        if key == 'map': \n",
    "            map_values = [v['map'].item() for v in values]  \n",
    "            map_50_values = [v['map_50'].item() for v in values]  \n",
    "\n",
    "            # Calculate mean and standard deviation for both 'map' and 'map_50'\n",
    "            map_mean = np.mean(map_values)\n",
    "            map_std_dev = np.std(map_values)\n",
    "            \n",
    "            map_50_mean = np.mean(map_50_values)\n",
    "            map_50_std_dev = np.std(map_50_values)\n",
    "\n",
    "            # Store results\n",
    "            results['map'] = {'mean': map_mean, 'std_dev': map_std_dev}\n",
    "            results['map_50'] = {'mean': map_50_mean, 'std_dev': map_50_std_dev}\n",
    "        else: \n",
    "            mean = np.mean(values)\n",
    "            std_dev = np.std(values)\n",
    "            results[key] = {'mean': mean, 'std_dev': std_dev}\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
